An accurate recognition of the user's emotional state is a primary aim of the human interface. The most pressing concern in the field of speech emotion identification is how to efficiently combine the extraction of suitable speech characteristics with a suitable classification engine in a parallel fashion. In this study, the concept of Emotion Recognition through speech signals involves predicting human emotions through speech with a high level of accuracy. This technology improves humancomputer interaction, although it is challenging to predict emotions due to their subjective nature and the difficulty of annotating audio. SER relies on various factors such as tone, pitch, expression, and behavior to determine emotions through speech. The process involves training classifiers with samples, and the RAVDESS dataset is used as an example in this work. Due to the wide range of vocal dynamics and pitch changes, emotion identification in spoken language is a difficult problem in computer vision. To overcome this, the Convolutional Neural Network (CNN) method is utilized for speech emotion detection; this employs emotion recognition modules and learners to determine the difference between states of happiness, surprise, anger, neutrality, and sorrow. The system's dataset is built from voice signals; the LIBROSA program is used to retrieve attributes from these samples. The highest precision may be attained through Adam optimization.
